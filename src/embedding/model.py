"""Embedding and ChromaDB vector storage.

Embeds document chunks using HuggingFace all-MiniLM-L6-v2 (384-dim)
via langchain-huggingface, then upserts them into a persistent
ChromaDB collection at corpus/vector_db/.

Chunk IDs are derived from the doc_id in corpus/metadata.csv for
full traceability back to the original download.  If a source file
has no matching metadata entry, a hash-based fallback ID is used.
"""

import csv
import hashlib
import logging
import os

import chromadb
from langchain_huggingface import HuggingFaceEmbeddings
from rich.console import Console
from rich.panel import Panel

from src.config import CFG, print_config
from src.utils import select_best_model

logger = logging.getLogger(__name__)

# ── Defaults ────────────────────────────────────────────────────────
MODEL_NAME = "all-MiniLM-L6-v2"
VECTOR_DB_DIR = os.path.join("corpus", "vector_db")
COLLECTION_NAME = str(CFG["collection_name"])
METADATA_CSV = os.path.join("corpus", "metadata.csv")
DEFAULT_BATCH_SIZE = 500

console = Console()


def print_ascii_banner():
    console.print(
        Panel.fit(
            """[bold dark_orange]
III N   N   JJJ EEEEE  SSSS TTTTT III  OOO  N   N 
 I  NN  N    J  E     S       T    I  O   O NN  N 
 I  N N N    J  EEEE   SSS    T    I  O   O N N N 
 I  N  NN J  J  E         S   T    I  O   O N  NN 
III N   N  JJ   EEEEE SSSS    T   III  OOO  N   N 
[/bold dark_orange]
------------------------------------------------
""",
            border_style="grey39",
        )
    )


# ── Helpers ─────────────────────────────────────────────────────────


def get_embedding_function(model_name: str | None = None) -> HuggingFaceEmbeddings:
    """Return a HuggingFaceEmbeddings instance.

    If *model_name* is None, the best model that fits in available
    RAM (with 30% overhead) is selected automatically via
    ``select_best_model()``.

    The model is downloaded on first use and cached in
    ~/.cache/huggingface/.
    """
    if model_name is None:
        selected = select_best_model()
        model_name = selected["name"]
        logger.info(f"Auto-selected embedding model: {model_name}")
    return HuggingFaceEmbeddings(model_name=model_name)


def get_or_create_collection(
    persist_dir: str = VECTOR_DB_DIR,
    collection_name: str = COLLECTION_NAME,
):
    """Open (or create) a persistent ChromaDB collection.

    Uses ChromaDB's embedded/in-process mode — no server required.
    Data is persisted to *persist_dir* on every write.
    """
    client = chromadb.PersistentClient(path=persist_dir)
    collection = client.get_or_create_collection(name=collection_name)
    logger.info(
        f"ChromaDB collection '{collection_name}' at {persist_dir}  "
        f"(existing docs: {collection.count()})"
    )
    return collection


def load_doc_id_map(metadata_csv: str = METADATA_CSV) -> dict:
    """Load corpus/metadata.csv into a lookup dict.

    Returns
    -------
    dict
        ``{local_path: {doc_id, title, author, source_url, ...}}``
        keyed by the ``local_path`` column so we can join on the
        chunk's ``source`` metadata field set by loaders.py.
    """
    doc_map: dict = {}
    if not os.path.exists(metadata_csv):
        logger.warning(
            f"Metadata CSV not found at {metadata_csv} — "
            "chunk IDs will use hash-based fallbacks."
        )
        return doc_map

    with open(metadata_csv, mode="r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            doc_map[row["local_path"]] = row

    logger.info(f"Loaded {len(doc_map)} entries from {metadata_csv}")
    return doc_map


def _fallback_doc_id(source: str) -> str:
    """Derive a deterministic fallback ID when the source is missing
    from metadata.csv.

    Uses the first 11 characters of the SHA-256 hex digest of the
    source path (matching the length of the Crockford Base32 IDs
    generated by downloader.py).
    """
    return hashlib.sha256(source.encode()).hexdigest()[:11]


# ── Core ────────────────────────────────────────────────────────────


def embed_and_store(
    chunks: list,
    batch_size: int = DEFAULT_BATCH_SIZE,
    persist_dir: str = VECTOR_DB_DIR,
    collection_name: str = COLLECTION_NAME,
    metadata_csv: str = METADATA_CSV,
) -> int:
    """Embed chunks and upsert them into ChromaDB.

    Parameters
    ----------
    chunks : list[Document]
        LangChain Document objects (from chunk_documents()).
    batch_size : int
        Max records per ChromaDB upsert call.  ChromaDB's SQLite
        backend limits the number of parameters per statement; 500 is
        a safe default well under the typical ~5,461 ceiling.
    persist_dir : str
        Path to the ChromaDB on-disk directory.
    collection_name : str
        Name of the ChromaDB collection.
    metadata_csv : str
        Path to corpus/metadata.csv for doc_id lookups.

    Returns
    -------
    int
        Total number of chunks upserted.
    """
    if not chunks:
        logger.warning("No chunks to embed.")
        return 0

    print_ascii_banner()
    print_config()

    # ── 1. Prepare lookup and models ────────────────────────────────
    doc_id_map = load_doc_id_map(metadata_csv)
    embedding_fn = get_embedding_function()
    collection = get_or_create_collection(persist_dir, collection_name)

    # ── 2. Build IDs, texts, metadatas ──────────────────────────────
    # Track per-source counters for sequential chunk numbering
    source_counters: dict[str, int] = {}
    ids: list[str] = []
    texts: list[str] = []
    metadatas: list[dict] = []

    for chunk in chunks:
        source = chunk.metadata.get("source", "unknown")

        # Sequential counter per source file
        n = source_counters.get(source, 0)
        source_counters[source] = n + 1

        # Look up doc_id from metadata CSV; fall back to hash
        csv_entry = doc_id_map.get(source)
        if csv_entry:
            doc_id = csv_entry["doc_id"]
        else:
            doc_id = _fallback_doc_id(source)
            logger.warning(
                f"No metadata.csv entry for '{source}' — using fallback ID '{doc_id}'"
            )

        chunk_id = f"{doc_id}_chunk_{n:04d}"
        ids.append(chunk_id)
        texts.append(chunk.page_content)

        # Enrich metadata: start with the chunk's own metadata,
        # then overlay fields from metadata.csv
        meta = dict(chunk.metadata)  # copy so we don't mutate original
        if csv_entry:
            meta["doc_id"] = csv_entry["doc_id"]
            meta["title"] = csv_entry.get("title", "")
            meta["category"] = csv_entry.get("category", "")
            meta["source_org"] = csv_entry.get("source_org", "")
            meta["author"] = csv_entry.get("author", "")
            meta["source_url"] = csv_entry.get("source_url", "")
            meta["source_type"] = csv_entry.get("source_type", "")

        metadatas.append(meta)

    logger.info(f"Embedding {len(texts)} chunk(s) …")

    # ── 3. Embed all texts at once ──────────────────────────────────
    embeddings = embedding_fn.embed_documents(texts)

    # ── 4. Batch upsert into ChromaDB ───────────────────────────────
    total = len(ids)
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        collection.upsert(
            ids=ids[start:end],
            embeddings=embeddings[start:end],
            documents=texts[start:end],
            metadatas=metadatas[start:end],
        )
        logger.info(f"  Upserted batch {start}–{end} of {total}")

    logger.info(
        f"Upserted {total} chunk(s) into collection '{collection_name}'  "
        f"(total in collection: {collection.count()})"
    )
    return total
